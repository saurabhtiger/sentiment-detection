{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3486 entries, 0 to 3485\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   review_id          3486 non-null   float64\n",
      " 1   translated_review  3481 non-null   object \n",
      " 2   actual_sentiment   3482 non-null   object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 81.8+ KB\n",
      "None Shape: (3486, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"D:/NLP/data/bank_a_sentiment_analysis_translated_inputdata.xlsx\")\n",
    "df_test=pd.read_excel(\"D:/NLP/data/bank_a_sentiment_analysis_translated_inputdata_unseen.xlsx\")\n",
    "print(df.info(),\"Shape:\",df.shape)\n",
    "df = df[[\"translated_review\",\"actual_sentiment\"]]\n",
    "df_test = df_test[[\"translated_review\",\"actual_sentiment\"]]\n",
    "# df=df[df[\"actual_sentiment\"]!=\"Neutral\"]\n",
    "# df_test=df_test[df_test[\"actual_sentiment\"]!=\"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we cannot impute text data. Therefore, removing empty/blank/NA values from dataset.\n",
    "df=df.dropna().reset_index()\n",
    "df=df.drop(\"index\",axis=1)\n",
    "df_test=df_test.dropna().reset_index()\n",
    "df_test=df_test.drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "\n",
    "    return \"\".join(cleantext)\n",
    "\n",
    "df['preprocessed_text']=df['translated_review'].map(lambda s:preprocess(s)) \n",
    "df_test['preprocessed_text']=df_test['translated_review'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAURAB~1.MIS\\AppData\\Local\\Temp/ipykernel_26432/4154870127.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['preprocessed_text'] = df['preprocessed_text'].str.replace('[^\\w\\s]','')\n",
      "C:\\Users\\SAURAB~1.MIS\\AppData\\Local\\Temp/ipykernel_26432/4154870127.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_test['preprocessed_text'] = df_test['preprocessed_text'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df['preprocessed_text'] = df['preprocessed_text'].str.replace('[^\\w\\s]','')\n",
    "df_test['preprocessed_text'] = df_test['preprocessed_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>actual_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch san marco evangelista employee include ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commission withdraw money counter theft</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f24 app thank</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low commission</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>promote support small business territory</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>availability rapidity</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>serious reliable comfort payment etc</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>norm</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>poor interest customer</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>experience positive customer close little prof...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3477 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text actual_sentiment\n",
       "0     branch san marco evangelista employee include ...         Positive\n",
       "1               commission withdraw money counter theft         Negative\n",
       "2                                         f24 app thank          Neutral\n",
       "3                                        low commission          Neutral\n",
       "4              promote support small business territory         Positive\n",
       "...                                                 ...              ...\n",
       "3472                              availability rapidity          Neutral\n",
       "3473               serious reliable comfort payment etc          Neutral\n",
       "3474                                               norm          Neutral\n",
       "3475                             poor interest customer         Negative\n",
       "3476  experience positive customer close little prof...         Negative\n",
       "\n",
       "[3477 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StopWords Removal\n",
    "from nltk.corpus import stopwords\n",
    "# stopwords = en.Defaults.stop_words\n",
    "stop = stopwords.words('english')\n",
    "not_stop=[\"no\",\"not\",\"too\"]\n",
    "for i in not_stop:\n",
    "    if i in stop:\n",
    "        stop.remove(i)\n",
    "df['clean_text'] = df['preprocessed_text'].apply(lambda x: \" \".join(i for i in x.split() if i not in stop))\n",
    "# Stemming\n",
    "df[\"cleaned_text\"]=df[\"clean_text\"].apply(lambda x: \" \".join(token.lemma_ for token in nlp(x) if not (token.like_num or token.is_punct or token.is_space or len(token)==1)))\n",
    "df=df[[\"cleaned_text\",\"actual_sentiment\"]]\n",
    "\n",
    "df_test['clean_text'] = df_test['preprocessed_text'].apply(lambda x: \" \".join(i for i in x.split() if i not in stop))\n",
    "# Stemming\n",
    "df_test[\"cleaned_text\"]=df_test[\"clean_text\"].apply(lambda x: \" \".join(token.lemma_ for token in nlp(x) if not (token.like_num or token.is_punct or token.is_space or len(token)==1)))\n",
    "df_test=df_test[[\"cleaned_text\",\"actual_sentiment\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['problem', 'banking', 'branch', 'helpful', 'customer', 'competent', 'feel', 'online', 'kind', 'efficient', 'staff', 'available', 'good', 'excellent', 'service']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['professionalit', 'cordiality', 'clarity', 'customer', 'professional', 'reliability', 'employee', 'seriousness', 'staff', 'professionalism', 'efficiency', 'competence', 'kindness', 'courtesy', 'availability']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['close', 'problem', 'safe', 'thank', 'employee', 'fine', 'consider', 'people', 'need', 'excellent', 'good', 'year', 'customer', 'reliable', 'bank']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def topic(df):\n",
    "    tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    dtm = tfidf.fit_transform(df['cleaned_text'])\n",
    "    nmf_model = NMF(n_components=3,random_state=42)\n",
    "    # This can take awhile, we're dealing with a large amount of documents!\n",
    "    nmf_model.fit(dtm)\n",
    "    for index,topic in enumerate(nmf_model.components_):\n",
    "        print(f'THE TOP 15 WORDS FOR TOPIC #{index+1}')\n",
    "        print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
    "        print('\\n')\n",
    "\n",
    "topic(df[df[\"actual_sentiment\"]==\"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['bad', 'offer', 'free', 'change', 'different', 'reliable', 'time', 'need', 'condition', 'account', 'recommend', 'make', 'compare', 'like', 'bank']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['compare', 'card', 'average', 'operation', 'transfer', 'expense', 'condition', 'commission', 'online', 'low', 'increase', 'account', 'management', 'high', 'cost']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['closure', 'change', 'problem', 'reject', 'remove', 'depend', 'long', 'hour', 'ask', 'year', 'opening', 'inadequate', 'atm', 'work', 'branch']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['employee', 'forget', 'despite', 'attention', 'relationship', 'need', 'feel', 'attentive', 'year', 'poor', 'small', 'assist', 'follow', 'available', 'customer']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['great', 'employee', 'clear', 'lot', 'feel', 'investment', 'worsen', 'dear', 'long', 'bad', 'poor', 'online', 'offer', 'good', 'service']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['old', 'previous', 'payment', 'expense', 'problem', 'like', 'work', 'account', 'change', 'improve', 'home', 'use', 'new', 'internet', 'banking']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['present', 'competitive', 'competent', 'flexibility', 'response', 'technological', 'director', 'problem', 'personal', 'available', 'slow', 'counter', 'availability', 'staff', 'little']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def topic(df):\n",
    "    tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    dtm = tfidf.fit_transform(df['cleaned_text'])\n",
    "    nmf_model = NMF(n_components=7,random_state=42)\n",
    "    # This can take awhile, we're dealing with a large amount of documents!\n",
    "    nmf_model.fit(dtm)\n",
    "    for index,topic in enumerate(nmf_model.components_):\n",
    "        print(f'THE TOP 15 WORDS FOR TOPIC #{index+1}')\n",
    "        print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
    "        print('\\n')\n",
    "\n",
    "topic(df[df[\"actual_sentiment\"]==\"Negative\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3477x1139 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 15131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = tfidf.fit_transform(df['cleaned_text'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(n_components=7, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(n_components=7, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=7,random_state=42)\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1139\n",
      "intend\n",
      "happy\n",
      "family\n",
      "moment\n",
      "feel\n",
      "office\n",
      "indecent\n",
      "milan\n",
      "expensive\n",
      "effectively\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features:\",len(tfidf.get_feature_names_out()))\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,len(tfidf.get_feature_names_out()))\n",
    "    print(tfidf.get_feature_names_out()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "currently\n",
      "city\n",
      "adequate\n",
      "unicredit\n",
      "dirty\n",
      "willingness\n",
      "cashier\n",
      "marina\n",
      "cordialit\n",
      "pro\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,1139)\n",
    "    print(tfidf.get_feature_names_out()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.42937892e-03, 4.38414819e-03, ...,\n",
       "        5.04364088e-05, 1.36242031e-05, 0.00000000e+00],\n",
       "       [7.65556302e-03, 0.00000000e+00, 9.57278786e-05, ...,\n",
       "        9.25257070e-03, 3.08580508e-05, 6.00033817e-04],\n",
       "       [5.63249968e-03, 2.46890182e-02, 4.59104084e-03, ...,\n",
       "        1.47801616e-03, 2.11728183e-03, 5.90738632e-02],\n",
       "       ...,\n",
       "       [6.17071655e-03, 1.17365926e-02, 0.00000000e+00, ...,\n",
       "        1.08678393e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 3.76979067e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.66376834e-04, 0.00000000e+00, 5.50584026e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(nmf_model.components_))\n",
    "nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['account', 'satisfied', 'simple', 'customer', 'quality', 'efficiency', 'internet', 'use', 'counter', 'banking', 'cost', 'offer', 'online', 'excellent', 'service']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['professionalit', 'professional', 'reliability', 'customer', 'clarity', 'employee', 'cordiality', 'seriousness', 'professionalism', 'staff', 'efficiency', 'competence', 'kindness', 'courtesy', 'availability']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['trust', 'cost', 'people', 'know', 'close', 'fine', 'work', 'consider', 'like', 'account', 'need', 'excellent', 'year', 'customer', 'bank']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['employee', 'people', 'courteous', 'prepared', 'solve', 'need', 'branch', 'problem', 'customer', 'professional', 'helpful', 'competent', 'kind', 'staff', 'available']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['internet', 'web', 'human', 'problem', 'continue', 'personal', 'employee', 'banking', 'year', 'service', 'condition', 'customer', 'relationship', 'feel', 'good']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['complete', 'condition', 'quick', 'banking', 'friendly', 'safe', 'courteous', 'convenient', 'fast', 'service', 'branch', 'comfortable', 'simple', 'modern', 'efficient']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['convenient', 'prepared', 'transparent', 'quite', 'internet', 'work', 'institute', 'punctual', 'employee', 'consider', 'comfortable', 'banking', 'safe', 'fast', 'reliable']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
